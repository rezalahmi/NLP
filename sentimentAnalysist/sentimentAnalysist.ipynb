{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dataset\n",
      "  Using cached dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting sqlalchemy<2.0.0,>=1.3.2\n",
      "  Downloading SQLAlchemy-1.4.50.tar.gz (8.5 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 437, in _error_catcher\n",
      "    yield\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"c:\\Users\\intellaptop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 458, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"c:\\Users\\intellaptop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 502, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"c:\\Users\\intellaptop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"c:\\Users\\intellaptop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"c:\\Users\\intellaptop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 228, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 182, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 323, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 183, in resolve\n",
      "    discovered_reqs.extend(self._resolve_one(requirement_set, req))\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 388, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 340, in _get_abstract_dist_for\n",
      "    abstract_dist = self.preparer.prepare_linked_requirement(req)\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 467, in prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 255, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 129, in get_http_url\n",
      "    from_path, content_type = _download_http_url(\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 282, in _download_http_url\n",
      "    for chunk in download.chunks:\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 168, in iter\n",
      "    for x in it:\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 64, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"c:\\Users\\intellaptop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py\", line 135, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 442, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n",
      "WARNING: You are using pip version 20.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'd:\\projects\\nlp\\.venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dataset\n",
      "  Using cached dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting banal>=1.0.1\n",
      "  Using cached banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
      "Collecting alembic>=0.6.2\n",
      "  Downloading alembic-1.13.0-py3-none-any.whl (230 kB)\n",
      "Collecting sqlalchemy<2.0.0,>=1.3.2\n",
      "  Downloading SQLAlchemy-1.4.50.tar.gz (8.5 MB)\n",
      "Requirement already satisfied: typing-extensions>=4 in d:\\projects\\nlp\\.venv\\lib\\site-packages (from alembic>=0.6.2->dataset) (4.8.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Using cached greenlet-3.0.1-cp39-cp39-win_amd64.whl (287 kB)\n",
      "Collecting MarkupSafe>=0.9.2\n",
      "  Downloading MarkupSafe-2.1.3-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Using legacy 'setup.py install' for sqlalchemy, since package 'wheel' is not installed.\n",
      "Installing collected packages: banal, greenlet, sqlalchemy, MarkupSafe, Mako, alembic, dataset\n",
      "    Running setup.py install for sqlalchemy: started\n",
      "    Running setup.py install for sqlalchemy: finished with status 'done'\n",
      "Successfully installed Mako-1.3.0 MarkupSafe-2.1.3 alembic-1.13.0 banal-1.0.6 dataset-1.6.2 greenlet-3.0.1 sqlalchemy-1.4.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000022E31E063D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/dataset/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000022E31E1D1F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/dataset/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000022E31E1D310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/dataset/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000022E31E1D550>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/dataset/\n",
      "WARNING: You are using pip version 20.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'd:\\projects\\nlp\\.venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\NLP\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 5.67k/5.67k [00:00<?, ?B/s]\n",
      "Downloading metadata: 100%|██████████| 3.33k/3.33k [00:00<00:00, 3.39MB/s]\n",
      "Downloading readme: 100%|██████████| 5.04k/5.04k [00:00<00:00, 3.54MB/s]\n",
      "Downloading data: 6.58MB [00:01, 4.78MB/s]/5 [00:00<?, ?it/s]\n",
      "Downloading data: 617kB [00:00, 2.45MB/s]1/5 [00:05<00:20,  5.03s/it]\n",
      "Downloading data: 205kB [00:00, 1.06MB/s]2/5 [00:09<00:13,  4.52s/it]\n",
      "Downloading data: 616kB [00:00, 1.43MB/s]3/5 [00:12<00:08,  4.12s/it]\n",
      "Downloading data: 482kB [00:01, 373kB/s] 4/5 [00:18<00:04,  4.59s/it]\n",
      "Downloading data files: 100%|██████████| 5/5 [00:23<00:00,  4.71s/it]\n",
      "Extracting data files: 100%|██████████| 5/5 [00:00<00:00, 141.33it/s]\n",
      "Generating train split: 100%|██████████| 13617/13617 [00:00<00:00, 17937.89 examples/s]\n",
      "Generating test_food split: 100%|██████████| 1344/1344 [00:00<00:00, 13679.24 examples/s]\n",
      "Generating test_movies split: 100%|██████████| 816/816 [00:00<00:00, 13723.42 examples/s]\n",
      "Generating validation_food split: 100%|██████████| 1330/1330 [00:00<00:00, 17085.84 examples/s]\n",
      "Generating validation_movies split: 100%|██████████| 360/360 [00:00<00:00, 16390.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"persiannlp/parsinlu_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 13617/13617 [00:00<00:00, 787169.05 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1344/1344 [00:00<00:00, 153692.80 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 816/816 [00:00<00:00, 118941.86 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1330/1330 [00:00<00:00, 164390.41 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 360/360 [00:00<00:00, 40558.42 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['review', 'review_id', 'example_id', 'excel_id', 'question', 'category', 'aspect', 'label', 'guid'],\n",
       "        num_rows: 13617\n",
       "    })\n",
       "    test_food: Dataset({\n",
       "        features: ['review', 'review_id', 'example_id', 'excel_id', 'question', 'category', 'aspect', 'label', 'guid'],\n",
       "        num_rows: 1344\n",
       "    })\n",
       "    test_movies: Dataset({\n",
       "        features: ['review', 'review_id', 'example_id', 'excel_id', 'question', 'category', 'aspect', 'label', 'guid'],\n",
       "        num_rows: 816\n",
       "    })\n",
       "    validation_food: Dataset({\n",
       "        features: ['review', 'review_id', 'example_id', 'excel_id', 'question', 'category', 'aspect', 'label', 'guid'],\n",
       "        num_rows: 1330\n",
       "    })\n",
       "    validation_movies: Dataset({\n",
       "        features: ['review', 'review_id', 'example_id', 'excel_id', 'question', 'category', 'aspect', 'label', 'guid'],\n",
       "        num_rows: 360\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'review': 'دوستان حتما دقت کنید درقسمت فیله تکه های سبز رنگ داشت که نشان دهنده انتیبیوتیک هایی هست که به مرغ ها میدن و اون تکه سمیه. بیشترشم استخونه من راضی نبودم و دیگه ازین مارو خرید نمیکنم',\n",
       " 'review_id': '1',\n",
       " 'example_id': '1',\n",
       " 'excel_id': 'food_1744',\n",
       " 'question': 'نظر شما در مورد عطر، بو، و طعم این گوشت مرغ چیست؟',\n",
       " 'category': 'گوشت مرغ',\n",
       " 'aspect': 'طعم',\n",
       " 'label': '-3',\n",
       " 'guid': 'food-train-r1-e1'}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df = dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13617 entries, 0 to 13616\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   review      13617 non-null  object\n",
      " 1   review_id   13617 non-null  object\n",
      " 2   example_id  13617 non-null  object\n",
      " 3   excel_id    13617 non-null  object\n",
      " 4   question    13617 non-null  object\n",
      " 5   category    13617 non-null  object\n",
      " 6   aspect      13617 non-null  object\n",
      " 7   label       13617 non-null  object\n",
      " 8   guid        13617 non-null  object\n",
      "dtypes: object(9)\n",
      "memory usage: 957.6+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...</td>\n",
       "      <td>-2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review label\n",
       "0  دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...    -3\n",
       "1  دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...    -3\n",
       "2  دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...    -3\n",
       "3  دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...    -3\n",
       "4  دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...    -2"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_df[['review','label']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13612</th>\n",
       "      <td>یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13613</th>\n",
       "      <td>یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13614</th>\n",
       "      <td>یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13615</th>\n",
       "      <td>یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...</td>\n",
       "      <td>-3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13616</th>\n",
       "      <td>یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review label\n",
       "13612  یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...    -3\n",
       "13613  یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...    -3\n",
       "13614  یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...    -3\n",
       "13615  یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...    -3\n",
       "13616  یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...    -1"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_df[['review','label']].tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['گوشت مرغ', 'شیر', 'ماهی، میگو و خاویار', 'چای', 'شربت و آبمیوه',\n",
       "       'بیسکویت و ویفر', 'تخمه و مغز طعم\\u200cدار', 'تخم مرغ', 'قهوه',\n",
       "       'نوشابه', 'شکلات، تافی و آبنبات', 'روغن', 'مربا', 'عسل',\n",
       "       'گوشت گاو و گوساله', 'تن ماهی', 'حلوا شکری، ارده و کنجد',\n",
       "       'آدامس و خوشبو کننده\\u200cی دهان', 'حبوبات و سویا',\n",
       "       'ماکارونی، پاستا و رشته', ' مردی بدون سایه',\n",
       "       ' چهارشنبه ۱۹ اردیبهشت', ' مسخره\\u200cباز', ' لانتوری',\n",
       "       ' سر به مهر', ' قسم', ' شنل', ' مردن به وقت شهریور',\n",
       "       ' قندون جهیزیه', ' ملی و راه\\u200cهای نرفته\\u200cاش',\n",
       "       ' زیر سقف دودی', ' غلامرضا تختی', ' نبات', ' حریم شخصی',\n",
       "       ' سلام بمبیی', ' سارا و آیدا', ' پنج ستاره', ' هفت ماهگی',\n",
       "       ' گشت ۲', ' دوران عاشقی', ' حوض نقاشی', ' آشنایی با لیلا',\n",
       "       ' خانه\\u200cای در خیابان چهل و یکم', ' حمال طلا', ' ابد و یک روز',\n",
       "       ' رگ خواب', ' زرد', ' آشغال\\u200cهای دوست\\u200cداشتنی', ' پاپ',\n",
       "       ' سیزده', ' ماهی و گربه', ' شب\\u200cهای روشن', ' قصر شیرین',\n",
       "       ' ویلایی\\u200cها', ' ناهید', ' متری شیش و نیم',\n",
       "       ' آینه\\u200cهای روبرو', ' تابو', ' ملبورن', ' مغزهای کوچک زنگ زده',\n",
       "       ' هت\\u200cتریک', ' نهنگ عنبر ۲ - سلکشن رویا', ' تراژدی',\n",
       "       ' تابستان داغ', ' گرگ بازی', ' به خاطر پونه',\n",
       "       'مستند چاووش، از درآمد تا فرود', ' بارکد', ' جاده قدیم',\n",
       "       'سریال دل', ' شکستن همزمان بیست استخوان', ' جان\\u200cدار',\n",
       "       ' فصل نرگس', ' خفه\\u200cگی', ' کلمبوس', ' او (خانه)', ' آستیگمات',\n",
       "       ' طلا', ' اکسیدان', ' دوباره زندگی', ' ایده اصلی', ' مطرب',\n",
       "       ' دختر', ' کفشهایم کو', ' تیک آف', ' شکاف', ' کوچه بی\\u200cنام',\n",
       "       ' برف', ' سد معبر', ' بوفالو', ' خشم و هیاهو', ' لس آنجلس تهران',\n",
       "       ' نقطه کور', ' ۵۰ کیلو آلبالو', ' سمفونی نهم',\n",
       "       ' همه چیز برای فروش', 'سریال سال\\u200cهای دور از خانه', ' اسرافیل',\n",
       "       ' خرگیوش', ' خانه کاغذی', ' رحمان ۱۴۰۰', ' سوتفاهم',\n",
       "       ' کارگر ساده نیازمندیم', ' فروشنده / The Salesman', ' تگزاس ۲',\n",
       "       'سریال نهنگ آبی', ' هفت معکوس', ' وارونگی', ' نزدیکتر',\n",
       "       ' چهار راه استانبول', ' ۳۶۰ درجه', ' داش آکل', ' آپاندیس',\n",
       "       'مستند در جستجوی فریده', ' آذر، شهدخت، پرویز و دیگران',\n",
       "       ' کلاس هنرپیشگی', ' سورنجان', ' احتمال باران اسیدی',\n",
       "       ' برف روی کاج\\u200cها', ' انارهای نارس', 'مستند میدان جوانان سابق',\n",
       "       ' شعله ور', ' جامه دران', ' ما همه با هم هستیم',\n",
       "       'مستند صحنه\\u200cهایی از یک جدایی', ' کاتیوشا', ' عرق سرد',\n",
       "       ' تیغ و ترمه', ' چند متر مکعب عشق', ' پشت دیوار سکوت',\n",
       "       ' آسمان زرد کم\\u200cعمق', ' عادت نمی\\u200cکنیم', ' سوفی و دیوانه',\n",
       "       ' ماجرای نیمروز ۲: رد خون', ' ماهورا', ' آندرانیک',\n",
       "       ' بدون تاریخ بدون امضا', ' درخونگاه', ' خوب، بد، جلف ۲: ارتش سری',\n",
       "       ' امیر', ' سرکوب', ' درساژ', ' لونه زنبور', ' جهان با من برقص',\n",
       "       ' بیگانه', ' دلم می\\u200cخواد', ' سال دوم دانشکده من',\n",
       "       ' کارت پرواز', ' هجوم', ' خوک', ' سرخپوست', ' خانوم',\n",
       "       ' شبی که ماه کامل شد', ' اعترافات ذهن خطرناک من', ' دریاچه ماهی',\n",
       "       ' با دیگران', ' بمب، یک عاشقانه', ' چاقی', ' خانه دختر',\n",
       "       ' چهارشنبه', 'سریال هیولا', ' چشم و گوش بسته', ' خانه دیگری',\n",
       "       ' خروج', ' زیر نظر', ' حق سکوت', ' خانم یایا', ' پارادایس', ' نفس',\n",
       "       ' کلوپ همسران'], dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_df['category'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13617 entries, 0 to 13616\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   review  13617 non-null  object\n",
      " 1   label   13617 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 212.9+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_data_df[['review', 'label']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 13617 entries, 0 to 13616\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   review      13617 non-null  object\n",
      " 1   review_id   13617 non-null  object\n",
      " 2   example_id  13617 non-null  object\n",
      " 3   excel_id    13617 non-null  object\n",
      " 4   question    13617 non-null  object\n",
      " 5   category    13617 non-null  object\n",
      " 6   aspect      13617 non-null  object\n",
      " 7   label       13617 non-null  int32 \n",
      " 8   guid        13617 non-null  object\n",
      "dtypes: int32(1), object(8)\n",
      "memory usage: 904.4+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_data_df['label'] = raw_data_df['label'].astype(int)\n",
    "raw_data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>review_id</th>\n",
       "      <th>example_id</th>\n",
       "      <th>excel_id</th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>aspect</th>\n",
       "      <th>label</th>\n",
       "      <th>guid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>food_1744</td>\n",
       "      <td>نظر شما در مورد عطر، بو، و طعم این گوشت مرغ چیست؟</td>\n",
       "      <td>گوشت مرغ</td>\n",
       "      <td>طعم</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>food-train-r1-e1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>food_1744</td>\n",
       "      <td>نظر شما در مورد قیمت و ارزش خرید این گوشت مرغ ...</td>\n",
       "      <td>گوشت مرغ</td>\n",
       "      <td>ارزش خرید</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>food-train-r1-e2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>food_1744</td>\n",
       "      <td>نظر شما در مورد ارسال و حمل و نقل این گوشت مرغ...</td>\n",
       "      <td>گوشت مرغ</td>\n",
       "      <td>ارسال</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>food-train-r1-e3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>food_1744</td>\n",
       "      <td>نظر شما در مورد بسته بندی و نگهداری این گوشت م...</td>\n",
       "      <td>گوشت مرغ</td>\n",
       "      <td>بسته بندی</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>food-train-r1-e4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>food_1744</td>\n",
       "      <td>نظر شما در مورد سلامت و ارزش غذایی این گوشت مر...</td>\n",
       "      <td>گوشت مرغ</td>\n",
       "      <td>ارزش غذایی</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>food-train-r1-e5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13612</th>\n",
       "      <td>یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...</td>\n",
       "      <td>359</td>\n",
       "      <td>4</td>\n",
       "      <td>movie_249</td>\n",
       "      <td>نظر شما در مورد فیلمبرداری و تصویربرداری فیلم ...</td>\n",
       "      <td>گشت ۲</td>\n",
       "      <td>فیلمبرداری</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>movie-train-r359-e4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13613</th>\n",
       "      <td>یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...</td>\n",
       "      <td>359</td>\n",
       "      <td>5</td>\n",
       "      <td>movie_249</td>\n",
       "      <td>نظر شما در مورد تهیه، تدوین، کارگردانی و ساخت ...</td>\n",
       "      <td>گشت ۲</td>\n",
       "      <td>کارگردانی</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>movie-train-r359-e5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13614</th>\n",
       "      <td>یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...</td>\n",
       "      <td>359</td>\n",
       "      <td>6</td>\n",
       "      <td>movie_249</td>\n",
       "      <td>نظر شما در مورد شخصیت پردازی، بازیگردانی و باز...</td>\n",
       "      <td>گشت ۲</td>\n",
       "      <td>بازی</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>movie-train-r359-e6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13615</th>\n",
       "      <td>یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...</td>\n",
       "      <td>359</td>\n",
       "      <td>7</td>\n",
       "      <td>movie_249</td>\n",
       "      <td>نظر شما در مورد گریم، طراحی صحنه و جلوه های وی...</td>\n",
       "      <td>گشت ۲</td>\n",
       "      <td>صحنه</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>movie-train-r359-e7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13616</th>\n",
       "      <td>یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...</td>\n",
       "      <td>359</td>\n",
       "      <td>8</td>\n",
       "      <td>movie_249</td>\n",
       "      <td>نظر شما به صورت کلی در مورد فیلم  گشت ۲ چیست؟</td>\n",
       "      <td>گشت ۲</td>\n",
       "      <td>کلی</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>movie-train-r359-e8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13617 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review review_id example_id  \\\n",
       "0      دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...         1          1   \n",
       "1      دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...         1          2   \n",
       "2      دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...         1          3   \n",
       "3      دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...         1          4   \n",
       "4      دوستان حتما دقت کنید درقسمت فیله تکه های سبز ر...         1          5   \n",
       "...                                                  ...       ...        ...   \n",
       "13612  یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...       359          4   \n",
       "13613  یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...       359          5   \n",
       "13614  یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...       359          6   \n",
       "13615  یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...       359          7   \n",
       "13616  یه فیلم تجاری دیگه که از گشت ارشاد هم ضعیفتره ...       359          8   \n",
       "\n",
       "        excel_id                                           question  category  \\\n",
       "0      food_1744  نظر شما در مورد عطر، بو، و طعم این گوشت مرغ چیست؟  گوشت مرغ   \n",
       "1      food_1744  نظر شما در مورد قیمت و ارزش خرید این گوشت مرغ ...  گوشت مرغ   \n",
       "2      food_1744  نظر شما در مورد ارسال و حمل و نقل این گوشت مرغ...  گوشت مرغ   \n",
       "3      food_1744  نظر شما در مورد بسته بندی و نگهداری این گوشت م...  گوشت مرغ   \n",
       "4      food_1744  نظر شما در مورد سلامت و ارزش غذایی این گوشت مر...  گوشت مرغ   \n",
       "...          ...                                                ...       ...   \n",
       "13612  movie_249  نظر شما در مورد فیلمبرداری و تصویربرداری فیلم ...     گشت ۲   \n",
       "13613  movie_249  نظر شما در مورد تهیه، تدوین، کارگردانی و ساخت ...     گشت ۲   \n",
       "13614  movie_249  نظر شما در مورد شخصیت پردازی، بازیگردانی و باز...     گشت ۲   \n",
       "13615  movie_249  نظر شما در مورد گریم، طراحی صحنه و جلوه های وی...     گشت ۲   \n",
       "13616  movie_249      نظر شما به صورت کلی در مورد فیلم  گشت ۲ چیست؟     گشت ۲   \n",
       "\n",
       "           aspect  label                 guid  \n",
       "0             طعم   -3.0     food-train-r1-e1  \n",
       "1       ارزش خرید   -3.0     food-train-r1-e2  \n",
       "2           ارسال   -3.0     food-train-r1-e3  \n",
       "3       بسته بندی   -3.0     food-train-r1-e4  \n",
       "4      ارزش غذایی   -2.0     food-train-r1-e5  \n",
       "...           ...    ...                  ...  \n",
       "13612  فیلمبرداری   -3.0  movie-train-r359-e4  \n",
       "13613   کارگردانی   -3.0  movie-train-r359-e5  \n",
       "13614        بازی   -3.0  movie-train-r359-e6  \n",
       "13615        صحنه   -3.0  movie-train-r359-e7  \n",
       "13616         کلی   -1.0  movie-train-r359-e8  \n",
       "\n",
       "[13617 rows x 9 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_df.where(raw_data_df['label']<0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>review_id</th>\n",
       "      <th>example_id</th>\n",
       "      <th>excel_id</th>\n",
       "      <th>question</th>\n",
       "      <th>category</th>\n",
       "      <th>aspect</th>\n",
       "      <th>label</th>\n",
       "      <th>guid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13612</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13613</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13614</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13615</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13616</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13617 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      review review_id example_id excel_id question category aspect  label  \\\n",
       "0        NaN       NaN        NaN      NaN      NaN      NaN    NaN    NaN   \n",
       "1        NaN       NaN        NaN      NaN      NaN      NaN    NaN    NaN   \n",
       "2        NaN       NaN        NaN      NaN      NaN      NaN    NaN    NaN   \n",
       "3        NaN       NaN        NaN      NaN      NaN      NaN    NaN    NaN   \n",
       "4        NaN       NaN        NaN      NaN      NaN      NaN    NaN    NaN   \n",
       "...      ...       ...        ...      ...      ...      ...    ...    ...   \n",
       "13612    NaN       NaN        NaN      NaN      NaN      NaN    NaN    NaN   \n",
       "13613    NaN       NaN        NaN      NaN      NaN      NaN    NaN    NaN   \n",
       "13614    NaN       NaN        NaN      NaN      NaN      NaN    NaN    NaN   \n",
       "13615    NaN       NaN        NaN      NaN      NaN      NaN    NaN    NaN   \n",
       "13616    NaN       NaN        NaN      NaN      NaN      NaN    NaN    NaN   \n",
       "\n",
       "      guid  \n",
       "0      NaN  \n",
       "1      NaN  \n",
       "2      NaN  \n",
       "3      NaN  \n",
       "4      NaN  \n",
       "...    ...  \n",
       "13612  NaN  \n",
       "13613  NaN  \n",
       "13614  NaN  \n",
       "13615  NaN  \n",
       "13616  NaN  \n",
       "\n",
       "[13617 rows x 9 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_df.where(raw_data_df['label']>0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as you can see this dataset has not normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dataset_config.yaml: 100%|██████████| 138/138 [00:00<00:00, 18.0kB/s]\n",
      "d:\\Projects\\NLP\\.venv\\lib\\site-packages\\huggingface_hub\\file_download.py:147: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\intellaptop\\.cache\\hezar. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading builder script: 100%|██████████| 2.08k/2.08k [00:00<00:00, 2.03MB/s]\n",
      "Downloading readme: 100%|██████████| 208/208 [00:00<?, ?B/s] \n",
      "Downloading data: 100%|██████████| 6.66M/6.66M [00:26<00:00, 252kB/s]\n",
      "Downloading data: 100%|██████████| 515k/515k [00:01<00:00, 393kB/s] \n",
      "Generating train split: 28602 examples [00:00, 30305.82 examples/s]\n",
      "Generating test split: 2315 examples [00:00, 30824.51 examples/s]\n",
      "Hezar (WARNING): This dataset requires a tokenizer to work. Provide it in config as `tokenizer_path` or set it manually as `dataset.tokenizer = your_tokenizer` after building the dataset.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'pad_token_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\NLP\\sentimentAnalysist\\sentimentAnalysist.ipynb Cell 16\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/NLP/sentimentAnalysist/sentimentAnalysist.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhezar\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/NLP/sentimentAnalysist/sentimentAnalysist.ipynb#X21sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m sentiment_dataset \u001b[39m=\u001b[39m Dataset\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mhezarai/sentiment-dksf\u001b[39;49m\u001b[39m\"\u001b[39;49m)  \u001b[39m# A TextClassificationDataset instance\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\hezar\\data\\datasets\\dataset.py:104\u001b[0m, in \u001b[0;36mDataset.load\u001b[1;34m(cls, hub_path, config_filename, split, **kwargs)\u001b[0m\n\u001b[0;32m    102\u001b[0m dataset_config \u001b[39m=\u001b[39m DatasetConfig\u001b[39m.\u001b[39mload(hub_path, filename\u001b[39m=\u001b[39mconfig_filename, repo_type\u001b[39m=\u001b[39mRepoType\u001b[39m.\u001b[39mDATASET)\n\u001b[0;32m    103\u001b[0m dataset_config\u001b[39m.\u001b[39mpath \u001b[39m=\u001b[39m hub_path\n\u001b[1;32m--> 104\u001b[0m dataset \u001b[39m=\u001b[39m build_dataset(dataset_config\u001b[39m.\u001b[39mname, config\u001b[39m=\u001b[39mdataset_config, split\u001b[39m=\u001b[39msplit, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    105\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\hezar\\builders.py:112\u001b[0m, in \u001b[0;36mbuild_dataset\u001b[1;34m(name, config, split, **kwargs)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    109\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mUnknown dataset name: `\u001b[39m\u001b[39m{\u001b[39;00mname\u001b[39m}\u001b[39;00m\u001b[39m`!\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mAvailable dataset names: \u001b[39m\u001b[39m{\u001b[39;00mavailable_datasets\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    111\u001b[0m config \u001b[39m=\u001b[39m config \u001b[39mor\u001b[39;00m datasets_registry[name]\u001b[39m.\u001b[39mconfig_class()\n\u001b[1;32m--> 112\u001b[0m dataset \u001b[39m=\u001b[39m datasets_registry[name]\u001b[39m.\u001b[39mmodule_class(config, split, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    113\u001b[0m \u001b[39mreturn\u001b[39;00m dataset\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\hezar\\data\\datasets\\text_classification_dataset.py:66\u001b[0m, in \u001b[0;36mTextClassificationDataset.__init__\u001b[1;34m(self, config, split, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extract_labels()\n\u001b[0;32m     65\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_tokenizer()\n\u001b[1;32m---> 66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_collator \u001b[39m=\u001b[39m TextPaddingDataCollator(\n\u001b[0;32m     67\u001b[0m     tokenizer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[0;32m     68\u001b[0m     max_length\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconfig\u001b[39m.\u001b[39;49mmax_length,\n\u001b[0;32m     69\u001b[0m )\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\hezar\\data\\data_collators.py:48\u001b[0m, in \u001b[0;36mTextPaddingDataCollator.__init__\u001b[1;34m(self, tokenizer, padding_type, padding_side, max_length, return_tensors)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_length \u001b[39m=\u001b[39m max_length\n\u001b[0;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_tensors \u001b[39m=\u001b[39m return_tensors\n\u001b[0;32m     47\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfield_to_pad_id_mapping \u001b[39m=\u001b[39m {\n\u001b[1;32m---> 48\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtoken_ids\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[0;32m     49\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenizer\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_type_id,\n\u001b[0;32m     50\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     51\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mspecial_tokens_mask\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m1\u001b[39m,\n\u001b[0;32m     52\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m0\u001b[39m,\n\u001b[0;32m     53\u001b[0m }\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m padding_type \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlongest\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m max_length \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     56\u001b[0m     logger\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m     57\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou passed `max_length` while also setting `padding_type` to `longest` which are \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mincompatible! Instead leave `max_length` as None or set `padding_type` to `max_length`! \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mIgnoring `max_length`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m     )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'pad_token_id'"
     ]
    }
   ],
   "source": [
    "from hezar.data import Dataset\n",
    "\n",
    "sentiment_dataset = Dataset.load(\"hezarai/sentiment-dksf\")  # A TextClassificationDataset instance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 6.66M/6.66M [00:26<00:00, 252kB/s]\n",
      "Downloading data: 100%|██████████| 515k/515k [00:01<00:00, 387kB/s] \n",
      "Generating train split: 28602 examples [00:00, 32131.93 examples/s]\n",
      "Generating test split: 2315 examples [00:00, 37515.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "sentiment_dataset = load_dataset(\"hezarai/sentiment-dksf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 28602\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 2315\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 28602/28602 [00:00<00:00, 1363616.02 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 2315/2315 [00:00<00:00, 247377.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "sentiment_dataset.save_to_disk('dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_df = sentiment_dataset['train'].to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28602 entries, 0 to 28601\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    28602 non-null  object\n",
      " 1   label   28602 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 447.0+ KB\n"
     ]
    }
   ],
   "source": [
    "raw_data_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>شیشه هاش همون روز اول شکست</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>تمیز -مرتب- چقدر پیک آن تایم و مؤدب …سپاس بسیا...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>نظر من هیچ گاه این محصول رو خریداری نکنید اگه ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>برنج ایرانی سفارش دادم افتضاح بود با هندی هیچ ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>سلام اصلا راضی نیستم  یعنی ارزش خرید نداره</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0                         شیشه هاش همون روز اول شکست      0\n",
       "1  تمیز -مرتب- چقدر پیک آن تایم و مؤدب …سپاس بسیا...      1\n",
       "2  نظر من هیچ گاه این محصول رو خریداری نکنید اگه ...      0\n",
       "3  برنج ایرانی سفارش دادم افتضاح بود با هندی هیچ ...      0\n",
       "4         سلام اصلا راضی نیستم  یعنی ارزش خرید نداره      0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2], dtype=int64)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_df.label.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing lapack_lite: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\NLP\\sentimentAnalysist\\sentimentAnalysist.ipynb Cell 24\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/NLP/sentimentAnalysist/sentimentAnalysist.ipynb#X33sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mhazm\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\hazm\\__init__.py:18\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhazm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m informal_words\n\u001b[0;32m     16\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhazm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m abbreviations\n\u001b[1;32m---> 18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhazm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence_tagger\u001b[39;00m \u001b[39mimport\u001b[39;00m IOBTagger\n\u001b[0;32m     19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhazm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence_tagger\u001b[39;00m \u001b[39mimport\u001b[39;00m SequenceTagger\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mhazm\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpos_tagger\u001b[39;00m \u001b[39mimport\u001b[39;00m POSTagger\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\hazm\\sequence_tagger.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpycrfsuite\u001b[39;00m \u001b[39mimport\u001b[39;00m Tagger\n\u001b[0;32m      9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpycrfsuite\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n\u001b[1;32m---> 10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[0;32m     13\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeatures\u001b[39m(sent, index):\n\u001b[0;32m     14\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"فهرست فیچرها را برمی‌گرداند.\"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\sklearn\\__init__.py:83\u001b[0m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# We are not importing the rest of scikit-learn during the build\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     \u001b[39m# process, as it may not be compiled yet\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     77\u001b[0m     \u001b[39m# later is linked to the OpenMP runtime to make it possible to introspect\u001b[39;00m\n\u001b[0;32m     78\u001b[0m     \u001b[39m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[39;00m\n\u001b[0;32m     79\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m (\n\u001b[0;32m     80\u001b[0m         __check_build,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m         _distributor_init,  \u001b[39m# noqa: F401\u001b[39;00m\n\u001b[0;32m     82\u001b[0m     )\n\u001b[1;32m---> 83\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m clone\n\u001b[0;32m     84\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m \u001b[39mimport\u001b[39;00m show_versions\n\u001b[0;32m     86\u001b[0m     __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     87\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     88\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    130\u001b[0m     ]\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\sklearn\\base.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n\u001b[0;32m     18\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m InconsistentVersionWarning\n\u001b[1;32m---> 19\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_metadata_requests\u001b[39;00m \u001b[39mimport\u001b[39;00m _MetadataRequester\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\sklearn\\utils\\__init__.py:22\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_bunch\u001b[39;00m \u001b[39mimport\u001b[39;00m Bunch\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_estimator_html_repr\u001b[39;00m \u001b[39mimport\u001b[39;00m estimator_html_repr\n\u001b[1;32m---> 22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_param_validation\u001b[39;00m \u001b[39mimport\u001b[39;00m Interval, validate_params\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mclass_weight\u001b[39;00m \u001b[39mimport\u001b[39;00m compute_class_weight, compute_sample_weight\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdeprecation\u001b[39;00m \u001b[39mimport\u001b[39;00m deprecated\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:15\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m \u001b[39mimport\u001b[39;00m csr_matrix, issparse\n\u001b[0;32m     14\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m config_context, get_config\n\u001b[1;32m---> 15\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mvalidation\u001b[39;00m \u001b[39mimport\u001b[39;00m _is_arraylike_not_scalar\n\u001b[0;32m     18\u001b[0m \u001b[39mclass\u001b[39;00m \u001b[39mInvalidParameterError\u001b[39;00m(\u001b[39mValueError\u001b[39;00m, \u001b[39mTypeError\u001b[39;00m):\n\u001b[0;32m     19\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Custom exception to be raised when the parameter of a class/method/function\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39m    does not have a valid type or value.\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config \u001b[39mas\u001b[39;00m _get_config\n\u001b[0;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mexceptions\u001b[39;00m \u001b[39mimport\u001b[39;00m DataConversionWarning, NotFittedError, PositiveSpectrumWarning\n\u001b[1;32m---> 25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_array_api\u001b[39;00m \u001b[39mimport\u001b[39;00m _asarray_with_order, _is_numpy_namespace, get_namespace\n\u001b[0;32m     26\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m ComplexWarning\n\u001b[0;32m     27\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_isfinite\u001b[39;00m \u001b[39mimport\u001b[39;00m FiniteStatus, cy_isfinite\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\sklearn\\utils\\_array_api.py:9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspecial\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mspecial\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_config\u001b[39;00m \u001b[39mimport\u001b[39;00m get_config\n\u001b[1;32m----> 9\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mfixes\u001b[39;00m \u001b[39mimport\u001b[39;00m parse_version\n\u001b[0;32m     12\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_check_array_api_dispatch\u001b[39m(array_api_dispatch):\n\u001b[0;32m     13\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check that array_api_compat is installed and NumPy version is compatible.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \n\u001b[0;32m     15\u001b[0m \u001b[39m    array_api_compat follows NEP29, which has a higher minimum NumPy version than\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[39m    scikit-learn.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\sklearn\\utils\\fixes.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msparse\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mstats\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mthreadpoolctl\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msklearn\u001b[39;00m\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\scipy\\stats\\__init__.py:608\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39m.. _statsrefmanual:\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    603\u001b[0m \n\u001b[0;32m    604\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    606\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_warnings_errors\u001b[39;00m \u001b[39mimport\u001b[39;00m (ConstantInputWarning, NearConstantInputWarning,\n\u001b[0;32m    607\u001b[0m                                DegenerateDataWarning, FitError)\n\u001b[1;32m--> 608\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_stats_py\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m    609\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_variation\u001b[39;00m \u001b[39mimport\u001b[39;00m variation\n\u001b[0;32m    610\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdistributions\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\scipy\\stats\\_stats_py.py:37\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mimport\u001b[39;00m array, asarray, ma\n\u001b[0;32m     36\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlib\u001b[39;00m \u001b[39mimport\u001b[39;00m NumpyVersion\n\u001b[1;32m---> 37\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtesting\u001b[39;00m \u001b[39mimport\u001b[39;00m suppress_warnings\n\u001b[0;32m     39\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mspatial\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdistance\u001b[39;00m \u001b[39mimport\u001b[39;00m cdist\n\u001b[0;32m     40\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mscipy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mndimage\u001b[39;00m \u001b[39mimport\u001b[39;00m _measurements\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\numpy\\testing\\__init__.py:11\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39munittest\u001b[39;00m \u001b[39mimport\u001b[39;00m TestCase\n\u001b[0;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m \u001b[39mimport\u001b[39;00m _private\n\u001b[1;32m---> 11\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[0;32m     12\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m (_assert_valid_refcount, _gen_alignment_data)\n\u001b[0;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m_private\u001b[39;00m \u001b[39mimport\u001b[39;00m extbuild, decorators \u001b[39mas\u001b[39;00m dec\n",
      "File \u001b[1;32md:\\Projects\\NLP\\.venv\\lib\\site-packages\\numpy\\testing\\_private\\utils.py:23\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m(\n\u001b[0;32m     22\u001b[0m      intp, float32, empty, arange, array_repr, ndarray, isnat, array)\n\u001b[1;32m---> 23\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlinalg\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlapack_lite\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mio\u001b[39;00m \u001b[39mimport\u001b[39;00m StringIO\n\u001b[0;32m     27\u001b[0m __all__ \u001b[39m=\u001b[39m [\n\u001b[0;32m     28\u001b[0m         \u001b[39m'\u001b[39m\u001b[39massert_equal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39massert_almost_equal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39massert_approx_equal\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     29\u001b[0m         \u001b[39m'\u001b[39m\u001b[39massert_array_equal\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39massert_array_less\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39massert_string_equal\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[39m'\u001b[39m\u001b[39m_OLD_PROMOTION\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     40\u001b[0m         ]\n",
      "\u001b[1;31mImportError\u001b[0m: DLL load failed while importing lapack_lite: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import hazm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Normalizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Projects\\NLP\\sentimentAnalysist\\sentimentAnalysist.ipynb Cell 24\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/NLP/sentimentAnalysist/sentimentAnalysist.ipynb#X32sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m punctuation \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m?.؟!،,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Projects/NLP/sentimentAnalysist/sentimentAnalysist.ipynb#X32sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m normalizer \u001b[39m=\u001b[39m Normalizer()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/NLP/sentimentAnalysist/sentimentAnalysist.ipynb#X32sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocessing\u001b[39m(item):\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Projects/NLP/sentimentAnalysist/sentimentAnalysist.ipynb#X32sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     output \u001b[39m=\u001b[39m normalizer\u001b[39m.\u001b[39mnormalize(item)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Normalizer' is not defined"
     ]
    }
   ],
   "source": [
    "punctuation = \"?.؟!،,\"\n",
    "normalizer = Normalizer()\n",
    "def preprocessing(item):\n",
    "    output = normalizer.normalize(item)\n",
    "    output = output.replace(\"\\_\", \"\\u200c\")\n",
    "#     output = output.replace(\"\\u200c\", \"\")\n",
    "    output = re.sub(r\"LINK([^ ])*|TAG|ID|\\-|@|LINK|[A-Za-z]\", \"\", output)\n",
    "    for i in punctuation:\n",
    "        p = \"\\\\\" + i + \"{2,}\"\n",
    "        output = re.sub(p, i, output)\n",
    "#     output = re.sub(\"\\?|\\.|\\؟|\\!|\\،|\\,\", \"\", output)\n",
    "    output = removeEmojies(output)\n",
    "    emoj = re.compile(e)\n",
    "    for i in emoj.findall(output):\n",
    "        allemojies.add(i)\n",
    "    output = re.sub(\"\\s+\", \" \", output).strip()\n",
    "    return output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
