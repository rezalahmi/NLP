{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dataset\n",
      "  Using cached dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting sqlalchemy<2.0.0,>=1.3.2\n",
      "  Downloading SQLAlchemy-1.4.50.tar.gz (8.5 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Exception:\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 437, in _error_catcher\n",
      "    yield\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 519, in read\n",
      "    data = self._fp.read(amt) if not fp_closed else b\"\"\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_vendor\\cachecontrol\\filewrapper.py\", line 62, in read\n",
      "    data = self.__fp.read(amt)\n",
      "  File \"c:\\Users\\intellaptop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 458, in read\n",
      "    n = self.readinto(b)\n",
      "  File \"c:\\Users\\intellaptop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\http\\client.py\", line 502, in readinto\n",
      "    n = self.fp.readinto(b)\n",
      "  File \"c:\\Users\\intellaptop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\socket.py\", line 704, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "  File \"c:\\Users\\intellaptop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\", line 1241, in recv_into\n",
      "    return self.read(nbytes, buffer)\n",
      "  File \"c:\\Users\\intellaptop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\ssl.py\", line 1099, in read\n",
      "    return self._sslobj.read(len, buffer)\n",
      "socket.timeout: The read operation timed out\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\cli\\base_command.py\", line 228, in _main\n",
      "    status = self.run(options, args)\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\cli\\req_command.py\", line 182, in wrapper\n",
      "    return func(self, options, args)\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\commands\\install.py\", line 323, in run\n",
      "    requirement_set = resolver.resolve(\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 183, in resolve\n",
      "    discovered_reqs.extend(self._resolve_one(requirement_set, req))\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 388, in _resolve_one\n",
      "    abstract_dist = self._get_abstract_dist_for(req_to_install)\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\resolution\\legacy\\resolver.py\", line 340, in _get_abstract_dist_for\n",
      "    abstract_dist = self.preparer.prepare_linked_requirement(req)\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 467, in prepare_linked_requirement\n",
      "    local_file = unpack_url(\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 255, in unpack_url\n",
      "    file = get_http_url(\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 129, in get_http_url\n",
      "    from_path, content_type = _download_http_url(\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\operations\\prepare.py\", line 282, in _download_http_url\n",
      "    for chunk in download.chunks:\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\cli\\progress_bars.py\", line 168, in iter\n",
      "    for x in it:\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_internal\\network\\utils.py\", line 64, in response_chunks\n",
      "    for chunk in response.raw.stream(\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 576, in stream\n",
      "    data = self.read(amt=amt, decode_content=decode_content)\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 541, in read\n",
      "    raise IncompleteRead(self._fp_bytes_read, self.length_remaining)\n",
      "  File \"c:\\Users\\intellaptop\\AppData\\Local\\Programs\\Python\\Python39\\lib\\contextlib.py\", line 135, in __exit__\n",
      "    self.gen.throw(type, value, traceback)\n",
      "  File \"d:\\projects\\nlp\\.venv\\lib\\site-packages\\pip\\_vendor\\urllib3\\response.py\", line 442, in _error_catcher\n",
      "    raise ReadTimeoutError(self._pool, None, \"Read timed out.\")\n",
      "pip._vendor.urllib3.exceptions.ReadTimeoutError: HTTPSConnectionPool(host='files.pythonhosted.org', port=443): Read timed out.\n",
      "WARNING: You are using pip version 20.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'd:\\projects\\nlp\\.venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dataset\n",
      "  Using cached dataset-1.6.2-py2.py3-none-any.whl (18 kB)\n",
      "Collecting banal>=1.0.1\n",
      "  Using cached banal-1.0.6-py2.py3-none-any.whl (6.1 kB)\n",
      "Collecting alembic>=0.6.2\n",
      "  Downloading alembic-1.13.0-py3-none-any.whl (230 kB)\n",
      "Collecting sqlalchemy<2.0.0,>=1.3.2\n",
      "  Downloading SQLAlchemy-1.4.50.tar.gz (8.5 MB)\n",
      "Requirement already satisfied: typing-extensions>=4 in d:\\projects\\nlp\\.venv\\lib\\site-packages (from alembic>=0.6.2->dataset) (4.8.0)\n",
      "Collecting Mako\n",
      "  Downloading Mako-1.3.0-py3-none-any.whl (78 kB)\n",
      "Collecting greenlet!=0.4.17\n",
      "  Using cached greenlet-3.0.1-cp39-cp39-win_amd64.whl (287 kB)\n",
      "Collecting MarkupSafe>=0.9.2\n",
      "  Downloading MarkupSafe-2.1.3-cp39-cp39-win_amd64.whl (17 kB)\n",
      "Using legacy 'setup.py install' for sqlalchemy, since package 'wheel' is not installed.\n",
      "Installing collected packages: banal, greenlet, sqlalchemy, MarkupSafe, Mako, alembic, dataset\n",
      "    Running setup.py install for sqlalchemy: started\n",
      "    Running setup.py install for sqlalchemy: finished with status 'done'\n",
      "Successfully installed Mako-1.3.0 MarkupSafe-2.1.3 alembic-1.13.0 banal-1.0.6 dataset-1.6.2 greenlet-3.0.1 sqlalchemy-1.4.50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000022E31E063D0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/dataset/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000022E31E1D1F0>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/dataset/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000022E31E1D310>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/dataset/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<pip._vendor.urllib3.connection.HTTPSConnection object at 0x0000022E31E1D550>: Failed to establish a new connection: [Errno 11001] getaddrinfo failed')': /simple/dataset/\n",
      "WARNING: You are using pip version 20.2.3; however, version 23.3.1 is available.\n",
      "You should consider upgrading via the 'd:\\projects\\nlp\\.venv\\scripts\\python.exe -m pip install --upgrade pip' command.\n"
     ]
    }
   ],
   "source": [
    "!pip install dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Projects\\NLP\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 5.67k/5.67k [00:00<?, ?B/s]\n",
      "Downloading metadata: 100%|██████████| 3.33k/3.33k [00:00<00:00, 3.39MB/s]\n",
      "Downloading readme: 100%|██████████| 5.04k/5.04k [00:00<00:00, 3.54MB/s]\n",
      "Downloading data: 6.58MB [00:01, 4.78MB/s]/5 [00:00<?, ?it/s]\n",
      "Downloading data: 617kB [00:00, 2.45MB/s]1/5 [00:05<00:20,  5.03s/it]\n",
      "Downloading data: 205kB [00:00, 1.06MB/s]2/5 [00:09<00:13,  4.52s/it]\n",
      "Downloading data: 616kB [00:00, 1.43MB/s]3/5 [00:12<00:08,  4.12s/it]\n",
      "Downloading data: 482kB [00:01, 373kB/s] 4/5 [00:18<00:04,  4.59s/it]\n",
      "Downloading data files: 100%|██████████| 5/5 [00:23<00:00,  4.71s/it]\n",
      "Extracting data files: 100%|██████████| 5/5 [00:00<00:00, 141.33it/s]\n",
      "Generating train split: 100%|██████████| 13617/13617 [00:00<00:00, 17937.89 examples/s]\n",
      "Generating test_food split: 100%|██████████| 1344/1344 [00:00<00:00, 13679.24 examples/s]\n",
      "Generating test_movies split: 100%|██████████| 816/816 [00:00<00:00, 13723.42 examples/s]\n",
      "Generating validation_food split: 100%|██████████| 1330/1330 [00:00<00:00, 17085.84 examples/s]\n",
      "Generating validation_movies split: 100%|██████████| 360/360 [00:00<00:00, 16390.22 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"persiannlp/parsinlu_sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 13617/13617 [00:00<00:00, 456852.22 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1344/1344 [00:00<00:00, 94479.92 examples/s] \n",
      "Saving the dataset (1/1 shards): 100%|██████████| 816/816 [00:00<00:00, 58868.44 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 1330/1330 [00:00<00:00, 190065.56 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 360/360 [00:00<00:00, 69688.90 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk('sentimentAnalysist\\dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
